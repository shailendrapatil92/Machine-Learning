{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics = []\n",
    "prim_key = -1\n",
    "vocabDict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create required directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directory = \"files\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "directory = \"vocab\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import topics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_topics_csv = pd.read_csv(\"reuters/all-topics-strings.lc.txt\", header = None)\n",
    "data_topics = np.matrix(data_topics_csv)\n",
    "\n",
    "for ptr in range(0, data_topics.shape[0]):\n",
    "    data_topics[ptr, 0] = data_topics[ptr, 0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print data_topics\n",
    "#for topic in data_topics[:,0]:\n",
    "#    vocab_file = open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Tag method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def project_extract_tag(temp_line, tag_name):\n",
    "    if tag_name == 'ID':\n",
    "        temp_id = temp_line.split('NEWID=\"')[1].replace('\">','')\n",
    "        return temp_id\n",
    "    \n",
    "    if tag_name == 'TOPICS':\n",
    "        temp_line = temp_line.replace('<TOPICS>','<D>').replace('</TOPICS>','</D>').replace('</D>','')\n",
    "        topics1 = temp_line.split('<D>')\n",
    "        topics1 = filter(None, topics1)\n",
    "        return topics1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalize method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def project_text_normalize(temp_line):\n",
    "    words=[]\n",
    "\n",
    "    ''' ### Convert to the lower case email contents### '''\n",
    "    temp_line = temp_line.lower()\n",
    "\n",
    "    ''' ### Strip the HTML tags ### '''\n",
    "    regex = re.compile(\"[<\\[^<>\\]+>]\")\n",
    "    temp_line = regex.sub('', temp_line)\n",
    "\n",
    "    ''' ### Process Numbers ### '''\n",
    "    regex = re.compile('[0-9]+')\n",
    "    temp_line = regex.sub('number', temp_line)\n",
    "\n",
    "    ''' ### Process URL ### '''\n",
    "    regex = re.compile('(http|https)://[\\S]*')\n",
    "    temp_line = regex.sub('httpaddr', temp_line)\n",
    "\n",
    "    ''' ### Process Email Address ### '''\n",
    "    regex = re.compile('[\\S]+@[\\S]+')\n",
    "    temp_line = regex.sub('emailaddr', temp_line)\n",
    "\n",
    "    ''' ### Process Dollar Sign ### '''\n",
    "    temp_line = re.sub('[$]+','dollar', temp_line)\n",
    "\n",
    "    ''' ### remove Puntuaution ### '''\n",
    "    temp_line = temp_line.translate(None, string.punctuation)\n",
    "\n",
    "    ''' ### TOkenize the list ### '''\n",
    "    words += temp_line.split(' ')\n",
    "\n",
    "    ''' ### Remove Non alpha Numeric ### '''\n",
    "    words = map(lambda x: re.sub('[^a-zA-z0-9]','',x), words)\n",
    "    words = filter(None, words)\n",
    "\n",
    "    ''' ### Stem the Strings ### '''\n",
    "    words = map(lambda x: PorterStemmer().stem(x), words)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add to dictionary method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def project_add_to_dict(vocab, topics):\n",
    "    global vocabDict\n",
    "    \n",
    "    for topic in topics:\n",
    "        loc = np.where(data_topics == topic)\n",
    "        word_list = vocabDict.get(loc[0][0])\n",
    "        \n",
    "        word_list_temp = []\n",
    "        \n",
    "        for word in vocab:\n",
    "            if word_list == None:\n",
    "                for ptr in range(0, len(vocab)):\n",
    "                    if vocab[ptr] not in vocab[0: ptr] and vocab[ptr] not in range(ptr + 1, len(vocab)):\n",
    "                        word_list_temp += [vocab[ptr]]\n",
    "                        \n",
    "                word_list = word_list_temp\n",
    "                break\n",
    "            \n",
    "            if word in word_list:\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                word_list += [word]\n",
    "        \n",
    "        vocabDict.update({str(loc[0][0]): word_list})\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save document method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def project_save_document(doc_id, vocab, topics):\n",
    "    project_add_to_dict(vocab, topics)\n",
    "    \n",
    "    output_file = open('files/doc_'+str(doc_id)+'.csv','w')\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerow(vocab)\n",
    "    \n",
    "    #print \"DATA TOPICS\", data_topics.shape\n",
    "    #print \"TOPICS\", topics\n",
    "    \n",
    "    Y = np.zeros((data_topics.shape[0], 1))\n",
    "    Y[np.where(data_topics == topics)[0]] = 1\n",
    "    Y = Y.flatten()\n",
    "    writer.writerow(Y)\n",
    "    \n",
    "    output_file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read datafile method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def project_read_file(temp_fileline):\n",
    "    \n",
    "    global prim_key\n",
    "    global topics\n",
    "    \n",
    "    for line_num in range(0, len(temp_fileline)):\n",
    "        temp_line = temp_fileline[line_num]\n",
    "        body = ''\n",
    "        if temp_line.startswith('<REUTERS'):\n",
    "            prim_key = project_extract_tag(temp_line.strip(), 'ID')\n",
    "        \n",
    "        if temp_line.startswith('<TOPICS>'):\n",
    "            topics = project_extract_tag(temp_line.strip(), 'TOPICS')\n",
    "         \n",
    "        if len(topics) == 0:\n",
    "            continue\n",
    "        \n",
    "        if temp_line.__contains__('<BODY>'):\n",
    "            while True ^ temp_line.__contains__('</BODY>'):\n",
    "                body += temp_line\n",
    "                line_num += 1\n",
    "                temp_line = temp_fileline[line_num]\n",
    "\n",
    "            body = body.split('<BODY>')[1].replace('Reuter','')\n",
    "            vocab = project_text_normalize(body)\n",
    "            project_save_document(prim_key, vocab, topics)\n",
    "            topics = []\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Vocab Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def project_save_vocab():\n",
    "    #print \"VOCAB DICT: \", vocabDict\n",
    "    #print \"DATA TOPICS: \", data_topics\n",
    "    for topic in data_topics[:,0]:\n",
    "        vocab_file = open('vocab/Vocab'+str(topic[0,0])+'.csv', 'w')\n",
    "        writer = csv.writer(vocab_file)\n",
    "        x = vocabDict.get(str(np.where(data_topics==topic)[0][0]))\n",
    "        #print \"TOPIC: \", topic\n",
    "        #print \"x: \", x\n",
    "        if x != None:\n",
    "            writer.writerow([topic[0,0]]+x)\n",
    "        else:\n",
    "            writer.writerow([topic[0,0]])\n",
    "        vocab_file.close()\n",
    "        \n",
    "    print \"Created Vocab Files.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing File: reut2-000.sgm\n",
      "Parsing File: reut2-001.sgm\n",
      "Parsing File: reut2-002.sgm\n",
      "Parsing File: reut2-003.sgm\n",
      "Parsing File: reut2-004.sgm\n",
      "Parsing File: reut2-005.sgm\n",
      "Parsing File: reut2-006.sgm\n",
      "Parsing File: reut2-007.sgm\n",
      "Parsing File: reut2-008.sgm\n",
      "Parsing File: reut2-009.sgm\n",
      "Parsing File: reut2-010.sgm\n",
      "Parsing File: reut2-011.sgm\n",
      "Parsing File: reut2-012.sgm\n",
      "Parsing File: reut2-013.sgm\n",
      "Parsing File: reut2-014.sgm\n",
      "Parsing File: reut2-015.sgm\n",
      "Parsing File: reut2-016.sgm\n",
      "Parsing File: reut2-017.sgm\n",
      "Parsing File: reut2-018.sgm\n",
      "Parsing File: reut2-019.sgm\n",
      "Parsing File: reut2-020.sgm\n",
      "Parsing File: reut2-021.sgm\n",
      "Created Vocab Files.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    files=['reut2-000.sgm','reut2-001.sgm','reut2-002.sgm','reut2-003.sgm','reut2-004.sgm','reut2-005.sgm','reut2-006.sgm','reut2-007.sgm','reut2-008.sgm','reut2-009.sgm','reut2-010.sgm','reut2-011.sgm','reut2-012.sgm','reut2-013.sgm','reut2-014.sgm','reut2-015.sgm','reut2-016.sgm','reut2-017.sgm','reut2-018.sgm','reut2-019.sgm','reut2-020.sgm','reut2-021.sgm']\n",
    "    #files=['reut2-000.sgm']\n",
    "    for temp_filename in files:\n",
    "        print \"Parsing File: \" + str(temp_filename)\n",
    "        temp_file = open(\"reuters/\"+temp_filename, 'r')\n",
    "        temp_fileline = temp_file.readlines()\n",
    "        project_read_file(temp_fileline)\n",
    "        temp_file.close()\n",
    "    \n",
    "    project_save_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
